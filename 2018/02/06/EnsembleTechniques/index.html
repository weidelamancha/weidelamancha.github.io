<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="technique," />










<meta name="description" content="IntroductionThis post aims to provide an overview of ensemble (also called committee-based learning, or learning multiple classifier systems) techniques used in machine learning.  Any ensemble method">
<meta name="keywords" content="technique">
<meta property="og:type" content="article">
<meta property="og:title" content="Ensemble Techniques">
<meta property="og:url" content="http://yoursite.com/2018/02/06/EnsembleTechniques/index.html">
<meta property="og:site_name" content="Studies of Data (currently under construction)">
<meta property="og:description" content="IntroductionThis post aims to provide an overview of ensemble (also called committee-based learning, or learning multiple classifier systems) techniques used in machine learning.  Any ensemble method">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-02-12T14:14:51.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ensemble Techniques">
<meta name="twitter:description" content="IntroductionThis post aims to provide an overview of ensemble (also called committee-based learning, or learning multiple classifier systems) techniques used in machine learning.  Any ensemble method">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/02/06/EnsembleTechniques/"/>





  <title>Ensemble Techniques | Studies of Data (currently under construction)</title>
  








  <!-- 我加的：
  <meta charset="utf-8">
  <link rel="stylesheet" href="mermaid.min.css"> -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Studies of Data (currently under construction)</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">A blog about data scicence techniques and applications in business and bio/medical research</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/06/EnsembleTechniques/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weidelamancha">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Studies of Data (currently under construction)">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Ensemble Techniques</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-06T00:00:00+08:00">
                2018-02-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This post aims to provide an overview of ensemble (also called committee-based learning, or learning multiple classifier systems) techniques used in machine learning. </p>
<p>Any ensemble method can be regarded as trying a specific way to decide the weights for combining the individual learners, and different ensemble methods can be regarded as different implementations of weighted averaging.</p>
<p>Benefit of ensemble:</p>
<ul>
<li><p>Reduce the risk of choosing a wrong hypothesis from hypotheses with same accuracy (problem of high variance)</p>
</li>
<li><p>Reduce the risk of choosing a wrong local optima of single learning algorithm (problem of high computational variance)</p>
</li>
<li><p>Expand the hypothesis space of representable functions (problem of high bias)</p>
</li>
</ul>
<a id="more"></a>
<p>Types of ensemble: </p>
<ul>
<li><p><strong><em>Homogeneous ensemble</em></strong>: ensemble methods use a single base learning algorithm to produce base learners of the same type</p>
</li>
<li><p><strong><em>Heterogeneous ensemble</em></strong>: ensemble methods use multiple learning algorithms to produce base learners of different types (also called individual learners or component learners)</p>
</li>
</ul>
<p>To get a good ensemble, it is generally believed that the base learners should be as accurate as possible, and as diverse as possible. </p>
<p>Ensemble learning can be used online by updating weak learners and has been applied to object tracking and intrusion detection, etc.</p>
<hr>
<h1 id="Core-knowledge-of-ensemble-methods"><a href="#Core-knowledge-of-ensemble-methods" class="headerlink" title="Core knowledge of ensemble methods"></a>Core knowledge of ensemble methods</h1><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting exploits the <strong>dependence</strong> between the base learners, since the overall performance can be boosted in a <strong>residual-decreasing</strong> way. Boosting works by training a set of learners <strong>sequentially</strong> and combining them for prediction, where the later learners focus more on the mistakes of the earlier learners.</p>
<p>General boosting procedure:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">	A((D_train)) --&gt; B[h1] </span><br><span class="line">	B --&gt; C&#123;e1, a1&#125;</span><br><span class="line">	C --&gt; D((D2))</span><br><span class="line">	D --&gt; E[h2]</span><br><span class="line">	E --&gt; F&#123;e2, a2&#125;</span><br><span class="line">	F --&gt; G((D3))</span><br><span class="line">	G --&gt; H[h3]</span><br><span class="line">	H --&gt; I&#123;e3, a3&#125;</span><br><span class="line">	I --&gt; J((D4))</span><br><span class="line">	B --&gt; K[H]</span><br><span class="line">	E --&gt; K</span><br><span class="line">	H --&gt; K</span><br><span class="line">	K --&gt; L((y_pred))</span><br><span class="line">	M((x_test)) --&gt; L</span><br></pre></td></tr></table></figure>
<p><em>Figure legend:<br>D: sample distribution; h: classifier; e: classification error of h; a: weight of h; H: ensembled classifier</em></p>
<h3 id="AdaBoost-algorithm-Freund-and-Schapire-1997"><a href="#AdaBoost-algorithm-Freund-and-Schapire-1997" class="headerlink" title="AdaBoost algorithm [Freund and Schapire, 1997]"></a>AdaBoost algorithm [Freund and Schapire, 1997]</h3><p>AdaBoost algorithm was designed as a classification algorithm for minimizing the misclassification error. It can be interpreted as a stagewise estimation procedure for fitting an additive logistic regression model.</p>
<ul>
<li><p>If base learning algorithm can learn with specified distributions, it is often accomplished by <strong><em>re-weighting</em></strong>: weighting training examples in each round according to the sample distribution.<br><strong>Pro:</strong> provides an option for <strong>boosting with restart</strong> as a sanity check to ensure that the current base learner is better than random guess.</p>
</li>
<li><p>If can’t, it can be accomplished by <strong><em>re-sampling</em></strong>: sampling training examples in each round according to the desired distribution.<br><strong>Pro:</strong> especially for multi-class tasks, the base learner that cannot pass the sanity check can be removed, and a new data sample can be generated on which a new base learner will be trained; in this way, the AdaBoost procedure can avoid the problem of early-termination far before the specified number of rounds.</p>
</li>
</ul>
<p>Theoretically, it is necessary to constrain the complexity of base learners as well as the number of learning rounds; otherwise AdaBoost will overfit. However, <strong>empirically it often doesn’t overfit</strong>, as test error often tends to decrease even after the training error reaches zero. </p>
<p>The bound of <strong><em>generalization error</em></strong> is relevant to the <strong><em>margin</em></strong> (the margin of the classifier h on the instance x, or in other words, the distance of x to the classification hyperplane of h) , the number of learning rounds and the complexity of base learners. Margin distribution is believed crucial to the generalization performance of AdaBoost. Average margin or median margin are suggested to be considered as measures to compare margin distributions.</p>
<h3 id="Variants-of-AdaBoost"><a href="#Variants-of-AdaBoost" class="headerlink" title="Variants of AdaBoost"></a>Variants of AdaBoost</h3><ul>
<li><p>Variants of AdaBoost developed by considering different <strong><em>surrogate loss functions</em></strong>:<br>– <strong><em>LogitBoost</em></strong> [Friedman et al., 2000] considers the log loss<br>– <strong><em>L2Boost</em></strong> [Buhlmann and Yu, 2003] considers the l2 loss</p>
</li>
<li><p>Variants of AdaBoost for multi-class classification:<br>– <strong><em>AdaBoost.M1</em></strong> [Freund and Schapire, 1997] in which base learners are multi-class learners instead of binary classifiers. This algorithm could not use binary classifiers, and requires every base learner has less than 1/2 multi-class 0/1-loss<br>– <strong><em>SAMME</em></strong> [Zhu et al., 2006], an improvement over AdaBoost.M1 with modifications of weight calculation<br>– <strong><em>AdaBoost.MH</em></strong> [Schapire and Singer, 1999] follows the one-versus-rest strategy. The real-value output H(x) rather than the crisp classification of each AdaBoost classifier is used to identify the most probable class<br>– <strong><em>AdaBoost.M2</em></strong> [Freund and Schapire, 1997] follows the one-versus-one strategy, which minimizes a pseudo-loss<br>– <strong><em>AdaBoost.MR</em></strong> [Schapire and Singer, 1999] generalizes AdaBoost.M2 and minimizes a ranking loss motivated by the fact that the highest ranked class is more likely to be the correct class<br><del>Binary classifiers obtained by one-versus-one decomposition can also be aggregated by voting, pairwise coupling, directed acyclic graph, etc.</del></p>
</li>
<li><p>AdaBoost algorithm has been observed to be very sensitive to <strong>noise</strong>. Because of AdaBoost’s exponential loss function, if an instance were not classified as its given label, the weight of this instance will increase drastically so that the instance to be classified according to the given label in the next round. Variants of AdaBoost are developed to increase noise tolerance by modifying weight updating rule for D:<br>– <strong><em>MadaBoost</em></strong> [Domingo and Watanabe, 2000] sets an upper limit on the weights<br>– <strong><em>FilterBoost</em></strong> [Bradley and Schapire, 2008] does not employ the exponential loss function used in AdaBoost, but adopts the log loss function. The increase of the instance weights is upper bounded by 1, similar to the weight depressing in MadaBoost, but smoother<br>– (<strong><em>BBM</em></strong> (Boosting-By-Majority) [Freund, 1995] -&gt; <strong><em>BrownBoost</em></strong> [Freund, 2001] -&gt;) <strong><em>RobustBoost</em></strong> [Freund, 2009] boosts the normalized classification margin</p>
</li>
</ul>
<h2 id="Bagging-a-parallel-ensemble-algorithm"><a href="#Bagging-a-parallel-ensemble-algorithm" class="headerlink" title="Bagging, a parallel ensemble algorithm"></a>Bagging, a parallel ensemble algorithm</h2><p>Bagging can deal with binary classification as well as multi-class classification. It exploits the <strong>independence</strong> between the base learners to achieve error reduction and parallel computing. The ensembled classifier H makes an error only when more than half of all independent base classifiers make errors. According to <strong><em>Hoeffding inequality</em></strong>, the generalization error reduces exponentially to the ensemble size T. The performance of Bagging converges as T grows large.</p>
<p>Base learners with less dependence can be obtained by introducing randomness in the learning process. </p>
<p>Two key ingredients of Bagging are bootstrap and aggregation:</p>
<ul>
<li><p><strong><em>Bootstrap sampling</em></strong> [Efron and Tibshirani, 1993] obtains data subsets for training the base learners. All data subsets have large overlap (63.2%) with the original data set</p>
</li>
<li><p><strong><em>Aggregating</em></strong> outputs of base learners: vote for classification and average for regression</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">	A((D_train = a, b, c)) --&gt; |bootstrap sampling 1|B((D_train1 = c, a, a))</span><br><span class="line">	A --&gt; |bootstrap sampling 2|C((D_train2 = a, b, b))</span><br><span class="line">	A --&gt; |bootstrap sampling 3|D((D_train3 = b, c, c))</span><br><span class="line">	B --&gt; E[h1]</span><br><span class="line">	C --&gt; F[h2]</span><br><span class="line">	D --&gt; G[h3]</span><br><span class="line">	E --&gt; H((y_pred1))</span><br><span class="line">	F --&gt; I((y_pred2))</span><br><span class="line">	G --&gt; J((y_pred3))</span><br><span class="line">	H --&gt; K(vote / average)</span><br><span class="line">	I --&gt; K</span><br><span class="line">	J --&gt; K</span><br><span class="line">	K --&gt; L((y_pred))</span><br><span class="line">	M((x_test)) --&gt; H</span><br><span class="line">	N((x_test)) --&gt; I</span><br><span class="line">	O((x_test)) --&gt; J</span><br></pre></td></tr></table></figure>
<p><strong><em>Out-of-bag</em></strong> examples (examples having not been used in training) can be used to estimate the generalization error of bagging. They can also be used for estimating posterior probability of each node of decision tree. </p>
<p>If a base learning algorithm is insensitive to perturbation on data subsets, the base learners trained may be quite similar, and thus combining them will not help improve the generalization performance. Such learners are called <strong><em>stable learners</em></strong>, e.g. decision stumps and k-nearest neighbor. Bagging should be used with <strong><em>unstable learners</em></strong>, e.g. unpruned decision tree.</p>
<p>Bagging can reduce the variance of higher-order components, yet not affect the linear components. This implies that Bagging is better applied with highly nonlinear learners, which tend to be unstable.</p>
<h3 id="Random-Forest-Breiman-2001"><a href="#Random-Forest-Breiman-2001" class="headerlink" title="Random Forest [Breiman, 2001]"></a>Random Forest [Breiman, 2001]</h3><p>Random Forest (RF) is an extension of Bagging, where the major difference with Bagging is the incorporation of randomized feature selection before the step of split selection. In terms of test error, RF is preferable than Bagging no matter whether decision trees are pruned or unpruned. </p>
<h3 id="Other-random-tree-ensembles"><a href="#Other-random-tree-ensembles" class="headerlink" title="Other random tree ensembles"></a>Other random tree ensembles</h3><ul>
<li><p><strong><em>VR-Tree ensemble</em></strong> [Liu et al., 2008a] generates random decision trees by randomizing both the feature selection and split selection processes. Parameter α (0&lt;α &lt;1) controls the degree of randomness, i.e., the probability of random feature selection and subsequent split selection at each node of the tree. <strong><em>Coalescence method</em></strong> in which α is randomly chosen from [0, 0.5] is often superior to RF and VR-Tree ensemble with fixed α’s</p>
</li>
<li><p><strong><em>iForest (Isolation Forest)</em></strong> [Liu et al., 2008a] is used for anomaly detection. Anomaly score of x is calculated based on adjusted expected path length through all random trees</p>
</li>
<li><p><strong><em>SCiForest (iForest with split selection criterion)</em></strong> [Liu et al., 2010] tries to get smoother decision boundaries by considering hyper-plane splits derived from the combination of original features, and uses split selection criterion instead of completely random selection to reduce the risk of falling into poor suboptimal solutions because of the complication of hyper-planes</p>
</li>
</ul>
<h2 id="Combination-methods"><a href="#Combination-methods" class="headerlink" title="Combination methods"></a>Combination methods</h2><h3 id="Averaging"><a href="#Averaging" class="headerlink" title="Averaging"></a>Averaging</h3><h4 id="Simple-averaging"><a href="#Simple-averaging" class="headerlink" title="Simple averaging"></a>Simple averaging</h4><p>Simple averaging obtains the combined output by averaging the outputs of individual learners directly. The expected ensemble error is smaller by a factor of T than the averaged error of the individual learners, if errors of base learners are uncorrelated, which is generally hard to achieve in ensemble learning.</p>
<h4 id="Weighted-averaging"><a href="#Weighted-averaging" class="headerlink" title="Weighted averaging"></a>Weighted averaging</h4><p>Weighted averaging obtains the combined output by averaging the outputs of individual learners with different weights implying different importance. Optimal weights can be obtained if errors of base learners are uncorrelated, which is generally hard to achieve in ensemble learning. With a large ensemble where a lot of weights to learn, weighted averaging can easily lead to overfitting.</p>
<p>Simple averaging is appropriate for combining learners with similar performances, whereas if the individual learners exhibit nonidentical strength, weighted averaging with unequal weights may achieve a better performance.</p>
<h3 id="Voting"><a href="#Voting" class="headerlink" title="Voting"></a>Voting</h3><p>Voting is the most popular and fundamental combination method for nominal outputs. The h(x) can take form of class label or class probability. class probabilities estimated by most classifiers are poor; however, combination methods based on class probabilities are often highly competitive to those based on crisp labels, especially after a careful calibration.</p>
<h4 id="Majority-voting"><a href="#Majority-voting" class="headerlink" title="Majority voting"></a>Majority voting</h4><p>Majority voting is the most popular voting method. Every classifier votes for one class label, and the final output class label is the one that receives more than half of the votes. If none of the class labels receives more than half of the votes, a rejection option will be given and the combined classifier makes no prediction.</p>
<h4 id="Plurality-voting"><a href="#Plurality-voting" class="headerlink" title="Plurality voting"></a>Plurality voting</h4><p>Plurality voting takes the class label which receives the largest number of votes as the final winner.</p>
<h4 id="Weighted-voting"><a href="#Weighted-voting" class="headerlink" title="Weighted voting"></a>Weighted voting</h4><p>Weighted voting gives higher weight to strong base learners among those with unequal performance. Because base learners are usually highly correlated, in real practice, it does not always lead to a performance better than majority voting.</p>
<h4 id="Soft-voting"><a href="#Soft-voting" class="headerlink" title="Soft voting"></a>Soft voting</h4><p>Soft voting is generally the choice if base learners produce class probability for homogeneous ensembles. </p>
<ul>
<li><p><strong><em>Simple soft voting</em></strong> generates the combined output by simply averaging all the individual outputs</p>
</li>
<li><p><strong><em>Weighted soft voting</em></strong> combines individual outputs with different weights. The weight can take form of:<br>– classifier-specific weight assigned to each classifier<br>– class-specific weight assigned to each classifier per class. For heterogeneous ensembles, the class probabilities generated by different types of learners usually cannot be compared directly without a careful calibration. In such situations, the class probability outputs are often converted to class label outputs, and then the voting methods for class labels can be applied<br>– weight assigned to each example of each class for each classifier (not often used). </p>
</li>
</ul>
<h3 id="Combining-by-learning"><a href="#Combining-by-learning" class="headerlink" title="Combining by learning"></a>Combining by learning</h3><h4 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h4><p>Stacking is a general procedure where a learner (1st-level learner, meta-learner) is trained to combine the individual learners (2nd-level learner).<br>– <em>Step 1</em>: train the 1st-level learners (often heterogeneous) using the part 1 of original training data set<br>– <em>Step 2</em>: generate a new data set: output of 1st-level learners with part 2 of original data<br>– <em>Step 3</em>: use the new data set to train the 2nd-level learner.<br>Some findings:<br>– Wolpert [1992]: it is crucial to consider the types of features for the new training data, and the types of learning algorithms for the second-level learner<br>– Breiman [1996]: non-negativity of coefficients of lease-square linear regression as 2nd-level learner helps<br>– Ting and Witten [1999]: use class probabilities instead of class labels as features for the new data as it takes into account also the confidences of individual classifiers<br>– Ting and Witten [1999]: use multi-response linear regression as 2nd-level learner<br>– Clarke [2003]: empirical results showed that stacking is more robust than Bayesian Model Averaging, and the latter is quite sensitive to model approximation error</p>
<h4 id="Infinite-ensemble"><a href="#Infinite-ensemble" class="headerlink" title="Infinite ensemble"></a>Infinite ensemble</h4><h4 id="Algebraic-methods"><a href="#Algebraic-methods" class="headerlink" title="Algebraic methods"></a>Algebraic methods</h4><h4 id="Behavior-knowledge-space-method"><a href="#Behavior-knowledge-space-method" class="headerlink" title="Behavior knowledge space method"></a>Behavior knowledge space method</h4><h4 id="Decision-Template-Method"><a href="#Decision-Template-Method" class="headerlink" title="Decision Template Method"></a>Decision Template Method</h4><h3 id="Relevant-methods"><a href="#Relevant-methods" class="headerlink" title="Relevant methods"></a>Relevant methods</h3><p>There are methods making use of multiple learners, which are trained on different sub-problems, or chosen upon different test instances. Therefore they are not recognized as ensemble combination methods.</p>
<h4 id="Error-correcting-output-codes-ECOC"><a href="#Error-correcting-output-codes-ECOC" class="headerlink" title="Error-correcting output codes (ECOC)"></a>Error-correcting output codes (ECOC)</h4><p>ECOC is used for multi-class problem based on combination of binary classifiers.</p>
<h4 id="Dynamic-classifier-selection-DCS"><a href="#Dynamic-classifier-selection-DCS" class="headerlink" title="Dynamic classifier selection (DCS)"></a>Dynamic classifier selection (DCS)</h4><p>DCS is a ‘soft combination’ method that trains multiple individual learners, and dynamically selects 1 learner for each test instance. The individual classifiers are evaluated on each partition so that the best-performing one for each partition is determined. In the testing stage, the test instance will be categorized into a partition and then classified by the corresponding best classifier. </p>
<h4 id="Mixture-of-experts-ME"><a href="#Mixture-of-experts-ME" class="headerlink" title="Mixture of experts (ME)"></a>Mixture of experts (ME)</h4><p>ME [Jacobs et al., 1991, Xu et al., 1995] works in a <strong><em>divide-and-conquer</em></strong> strategy where a complex task is broken up into several simpler and smaller subtasks, and individual learners (experts) are trained for different subtasks. <strong><em>Gating</em></strong> is usually employed to combine the experts. In ME, a key problem is how to find the natural division of the task and then derive the overall solution from sub-solutions. A basic method is to target each expert to a distribution specified by the gating function, rather than the whole original training data distribution. The training procedure tries to achieve two goals: for given experts, to find the optimal gating function; for given gating function, to train the experts on the distribution specified by the gating function. The unknown parameters are usually estimated by the <strong><em>Expectation Maximization (EM)</em></strong> algorithm.</p>
<h2 id="Ensemble-diversity"><a href="#Ensemble-diversity" class="headerlink" title="Ensemble diversity"></a>Ensemble diversity</h2><p>it is desired that the individual learners should be accurate and diverse. The success of ensemble learning lies in achieving a good tradeoff between the individual performance and diversity, and complementarity is more important than pure accuracy. Currently there is no well-accepted formal definition of diversity</p>
<h3 id="Error-decomposition"><a href="#Error-decomposition" class="headerlink" title="Error decomposition"></a>Error decomposition</h3><h4 id="Error-ambiguity-decomposition"><a href="#Error-ambiguity-decomposition" class="headerlink" title="Error-ambiguity decomposition"></a>Error-ambiguity decomposition</h4><p>[Krogh and Vedelsby, 1995]:<br>Error of ensemble = Weighted average of individual generalization errors (ensemble error, or error of the individual leaners) - Weighted average of ambiguities (variability among predictions of individual leaners depending on the ensemble diversity)<br><strong>Limitation</strong>: </p>
<ul>
<li>It is derived only for regression and not classification</li>
<li>It is difficult to estimate weighted average of ambiguities empirically. The estimate of weighted average of ambiguities is obtained by subtracting the estimated value of ensemble errors from the estimated value of individual error </li>
<li>Calculated weighted average of ambiguities are not always positive as it should be</li>
</ul>
<h4 id="Bias-Variance-Covariance-Decomposition"><a href="#Bias-Variance-Covariance-Decomposition" class="headerlink" title="Bias-Variance-Covariance Decomposition"></a>Bias-Variance-Covariance Decomposition</h4><p>The bias-variance-covariance decomposition [Geman et al., 1992] is an important general tool for analyzing the performance of learning algorithms.<br>Error of ensemble = Averaged bias ^2 + Averaged Variance / T + Averaged covariance * (1 - 1/T)</p>
<ul>
<li><strong><em>Bias</em></strong>: measures how closely the average estimate of the learning algorithm is able to approximate the target. It often includes intrinsic noise, which is a lower bound on the expected error of any learning algorithm on the target</li>
<li><strong><em>Variance</em></strong>: measures how much the estimate of the learning approach fluctuates for different training sets of the same size</li>
<li><strong><em>Covariance</em></strong>: models the correlation between individual learners</li>
</ul>
<p>It is preferred that the individual learners make different errors.</p>
<p><strong>Limitation</strong>: </p>
<ul>
<li>It is derived only for regression and not classification</li>
<li>The covariance term can be negative</li>
</ul>
<p><strong>Conclusion</strong>: It is hard to maximize the ambiguity term without affecting the bias term, implying that generating diverse learners is a challenging problem.</p>
<h3 id="Diversity-measures"><a href="#Diversity-measures" class="headerlink" title="Diversity measures"></a>Diversity measures</h3><h4 id="Pairwise-measures-using-contingency-table"><a href="#Pairwise-measures-using-contingency-table" class="headerlink" title="Pairwise measures (using contingency table)"></a>Pairwise measures (using contingency table)</h4><p>To measure ensemble diversity, a classical approach is to measure the pairwise similarity/dissimilarity between two learners, and then average all the pairwise measurements for the overall diversity.</p>
<ul>
<li>Disagreement measure [Skalak, 1996, Ho, 1998]</li>
<li>Q-statistic</li>
<li>Correlation coefficient</li>
<li>Kappa-statistic</li>
<li>Double-fault measure [Giacinto and Roli, 2001]: requires known correctness of classification)</li>
</ul>
<h4 id="Non-pairwise-measures-assessing-ensemble-diversity-directly"><a href="#Non-pairwise-measures-assessing-ensemble-diversity-directly" class="headerlink" title="Non-pairwise measures (assessing ensemble diversity directly)"></a>Non-pairwise measures (assessing ensemble diversity directly)</h4><ul>
<li>Kohavi-Wolpert variance [Kohavi and Wolpert, 1996]</li>
<li>Interrater agreement [Kuncheva and Whitaker, 2003] </li>
<li>Entropy [Cunningham and Carney, 2000]: motivated by the fact that for an instance, the disagreement will be maximized if a tie occurs in the votes of individual classifiers</li>
<li>Difficulty [Kuncheva and Whitaker, 2003]</li>
<li>Generalized Diversity [Partridge and Krzanowski, 1997]: motivated by the argument that the diversity is maximized when the failure of one classifier is accompanied by the correct prediction of the other</li>
<li>Coincident Failure [Partridge and Krzanowski, 1997]: a modified version of the generalized diversity</li>
</ul>
<p><strong>Limitation</strong>:</p>
<ul>
<li>No clear relation between those diversity measurements and the ensemble performance</li>
<li>Maximum diversity is usually not achievable</li>
<li>Change of existing diversity measurements does not provide consistent guidance on whether an ensemble achieves good generalization performance</li>
<li>The diversity measurements are closely related to the average individual accuracies, which is undesirable since it is not expected that the diversity measure becomes another estimate of accuracy</li>
</ul>
<h3 id="Information-theoretic-diversity"><a href="#Information-theoretic-diversity" class="headerlink" title="Information theoretic diversity"></a>Information theoretic diversity</h3><p>Based on the concept of entropy, the dependence between two variables , x1 and x2 given y, can be measured by the <strong><em>conditional mutual information</em></strong>. Based on information theory, probability of error can be bounded by two inequalities [Brown, 2009]. To minimize the prediction error, the conditioned mutual information should be maximized. Two multivariate generalization of mutual information:</p>
<h4 id="Interaction-information-diversity"><a href="#Interaction-information-diversity" class="headerlink" title="Interaction information diversity"></a>Interaction information diversity</h4><p><strong>Limitation</strong>: The expression of the diversity terms, especially the involved interaction information, are quite complicated and there is no effective process for estimating them at multiple orders in practice.</p>
<h4 id="Multi-information-diversity"><a href="#Multi-information-diversity" class="headerlink" title="Multi-information diversity"></a>Multi-information diversity</h4><p>Multi-information diversity is mathematically equivalent to interaction information diversity, but has a much simpler formulation and decomposable terms over individual classifiers.</p>
<h3 id="Diversity-generation"><a href="#Diversity-generation" class="headerlink" title="Diversity generation"></a>Diversity generation</h3><h4 id="Data-sample-manipulation"><a href="#Data-sample-manipulation" class="headerlink" title="Data sample manipulation"></a>Data sample manipulation</h4><p>Based on sampling approaches, e.g. Bagging adopts bootstrap sampling, AdaBoost adopts sequential sampling, new learners uses artificial training data [Melville and Mooney 2005].</p>
<h4 id="Input-feature-manipulation"><a href="#Input-feature-manipulation" class="headerlink" title="Input feature manipulation"></a>Input feature manipulation</h4><p>Individual learners trained from different subspaces are usually diverse, e.g. Random Subspace method [Ho, 1998]. While Random Subspace is not suitable for data with only a few features, it is especially effective for data with redundant features. If there are lots of irrelevant features, it is usually better to filter out most irrelevant features before generating the subspaces.</p>
<h4 id="Learning-parameter-manipulation"><a href="#Learning-parameter-manipulation" class="headerlink" title="Learning parameter manipulation"></a>Learning parameter manipulation</h4><p>Diverse individual learners can be trained by using different parameter settings for the base learning algorithm. For example, different initial weights, split selections, candidate rule conditions.</p>
<h4 id="Output-representation-manipulation"><a href="#Output-representation-manipulation" class="headerlink" title="Output representation manipulation"></a>Output representation manipulation</h4><p>Diverse individual learners can be trained by using different output representations, e.g. ECOC approach, Flipping Output method [Breiman, 2000], Output Smearing method [Breiman, 2000].</p>
<hr>
<h1 id="Advanced-knowledge-of-ensemble-methods"><a href="#Advanced-knowledge-of-ensemble-methods" class="headerlink" title="Advanced knowledge of ensemble methods"></a>Advanced knowledge of ensemble methods</h1><h2 id="Ensemble-pruning"><a href="#Ensemble-pruning" class="headerlink" title="Ensemble pruning"></a>Ensemble pruning</h2><h2 id="Clustering-ensembles"><a href="#Clustering-ensembles" class="headerlink" title="Clustering ensembles"></a>Clustering ensembles</h2><h2 id="Ensemble-methods-in-semi-supervised-learning-active-learning-cost-sensitive-learning-and-class-imbalance-learning-as-well-as-comprehensibility-enhancement"><a href="#Ensemble-methods-in-semi-supervised-learning-active-learning-cost-sensitive-learning-and-class-imbalance-learning-as-well-as-comprehensibility-enhancement" class="headerlink" title="Ensemble methods in semi-supervised learning, active learning, cost-sensitive learning and class-imbalance learning, as well as comprehensibility enhancement"></a>Ensemble methods in semi-supervised learning, active learning, cost-sensitive learning and class-imbalance learning, as well as comprehensibility enhancement</h2><hr>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/technique/" rel="tag"># technique</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/05/HelloWorld/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">weidelamancha</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Core-knowledge-of-ensemble-methods"><span class="nav-number">2.</span> <span class="nav-text">Core knowledge of ensemble methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">2.1.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost-algorithm-Freund-and-Schapire-1997"><span class="nav-number">2.1.1.</span> <span class="nav-text">AdaBoost algorithm [Freund and Schapire, 1997]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variants-of-AdaBoost"><span class="nav-number">2.1.2.</span> <span class="nav-text">Variants of AdaBoost</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging-a-parallel-ensemble-algorithm"><span class="nav-number">2.2.</span> <span class="nav-text">Bagging, a parallel ensemble algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Random-Forest-Breiman-2001"><span class="nav-number">2.2.1.</span> <span class="nav-text">Random Forest [Breiman, 2001]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-random-tree-ensembles"><span class="nav-number">2.2.2.</span> <span class="nav-text">Other random tree ensembles</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Combination-methods"><span class="nav-number">2.3.</span> <span class="nav-text">Combination methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Averaging"><span class="nav-number">2.3.1.</span> <span class="nav-text">Averaging</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple-averaging"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">Simple averaging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weighted-averaging"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">Weighted averaging</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Voting"><span class="nav-number">2.3.2.</span> <span class="nav-text">Voting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Majority-voting"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">Majority voting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Plurality-voting"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">Plurality voting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weighted-voting"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">Weighted voting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Soft-voting"><span class="nav-number">2.3.2.4.</span> <span class="nav-text">Soft voting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Combining-by-learning"><span class="nav-number">2.3.3.</span> <span class="nav-text">Combining by learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Stacking"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">Stacking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Infinite-ensemble"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">Infinite ensemble</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algebraic-methods"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">Algebraic methods</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Behavior-knowledge-space-method"><span class="nav-number">2.3.3.4.</span> <span class="nav-text">Behavior knowledge space method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision-Template-Method"><span class="nav-number">2.3.3.5.</span> <span class="nav-text">Decision Template Method</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relevant-methods"><span class="nav-number">2.3.4.</span> <span class="nav-text">Relevant methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Error-correcting-output-codes-ECOC"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">Error-correcting output codes (ECOC)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dynamic-classifier-selection-DCS"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">Dynamic classifier selection (DCS)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mixture-of-experts-ME"><span class="nav-number">2.3.4.3.</span> <span class="nav-text">Mixture of experts (ME)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-diversity"><span class="nav-number">2.4.</span> <span class="nav-text">Ensemble diversity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Error-decomposition"><span class="nav-number">2.4.1.</span> <span class="nav-text">Error decomposition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Error-ambiguity-decomposition"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">Error-ambiguity decomposition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias-Variance-Covariance-Decomposition"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">Bias-Variance-Covariance Decomposition</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Diversity-measures"><span class="nav-number">2.4.2.</span> <span class="nav-text">Diversity measures</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pairwise-measures-using-contingency-table"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">Pairwise measures (using contingency table)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-pairwise-measures-assessing-ensemble-diversity-directly"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">Non-pairwise measures (assessing ensemble diversity directly)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-theoretic-diversity"><span class="nav-number">2.4.3.</span> <span class="nav-text">Information theoretic diversity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Interaction-information-diversity"><span class="nav-number">2.4.3.1.</span> <span class="nav-text">Interaction information diversity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-information-diversity"><span class="nav-number">2.4.3.2.</span> <span class="nav-text">Multi-information diversity</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Diversity-generation"><span class="nav-number">2.4.4.</span> <span class="nav-text">Diversity generation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-sample-manipulation"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">Data sample manipulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Input-feature-manipulation"><span class="nav-number">2.4.4.2.</span> <span class="nav-text">Input feature manipulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-parameter-manipulation"><span class="nav-number">2.4.4.3.</span> <span class="nav-text">Learning parameter manipulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Output-representation-manipulation"><span class="nav-number">2.4.4.4.</span> <span class="nav-text">Output representation manipulation</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Advanced-knowledge-of-ensemble-methods"><span class="nav-number">3.</span> <span class="nav-text">Advanced knowledge of ensemble methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-pruning"><span class="nav-number">3.1.</span> <span class="nav-text">Ensemble pruning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering-ensembles"><span class="nav-number">3.2.</span> <span class="nav-text">Clustering ensembles</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-methods-in-semi-supervised-learning-active-learning-cost-sensitive-learning-and-class-imbalance-learning-as-well-as-comprehensibility-enhancement"><span class="nav-number">3.3.</span> <span class="nav-text">Ensemble methods in semi-supervised learning, active learning, cost-sensitive learning and class-imbalance learning, as well as comprehensibility enhancement</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weidelamancha</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>


  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
